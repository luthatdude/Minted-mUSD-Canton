# Minted Protocol — Log Aggregation Stack (Loki + Promtail)
# GAP-8: Previously no centralized log aggregation manifest.
# This deploys Grafana Loki for log storage and Promtail as DaemonSet
# to collect logs from all pods in the musd-canton namespace.
#
# Architecture:
#   Promtail (DaemonSet) → Loki (StatefulSet) → Grafana (existing)
#   All pods' stdout/stderr are scraped, labeled, and indexed.
#
# Usage:
#   kubectl apply -f k8s/monitoring/loki-stack.yaml
#   Then add Loki as a datasource in Grafana: http://loki.musd-canton:3100
---
# ══════════════════════════════════════════════════════════════════
# 1. LOKI — Log Storage & Query Engine
# ══════════════════════════════════════════════════════════════════
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/component: log-storage
data:
  loki.yaml: |
    # INFRA-H-03: Enable multi-tenant authentication.
    # All clients must provide X-Scope-OrgID header (Promtail already sends tenant_id: minted).
    auth_enabled: true

    server:
      http_listen_port: 3100
      log_level: warn

    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory

    schema_config:
      configs:
        - from: "2026-01-01"
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

    limits_config:
      # Retention: 30 days of logs
      retention_period: 720h
      max_query_length: 720h
      # Ingestion limits per tenant
      ingestion_rate_mb: 10
      ingestion_burst_size_mb: 20
      per_stream_rate_limit: 5MB
      max_entries_limit_per_query: 10000

    compactor:
      working_directory: /loki/compactor
      compaction_interval: 10m
      retention_enabled: true
      delete_request_store: filesystem

    analytics:
      reporting_enabled: false
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: loki
    app.kubernetes.io/component: log-storage
spec:
  replicas: 1
  serviceName: loki
  selector:
    matchLabels:
      app.kubernetes.io/name: loki
  template:
    metadata:
      labels:
        app.kubernetes.io/name: loki
    spec:
      serviceAccountName: loki
      securityContext:
        runAsNonRoot: true
        runAsUser: 10001
        runAsGroup: 10001
        fsGroup: 10001
      containers:
        - name: loki
          image: grafana/loki:3.3.2@sha256:4bb8054e02f2c3dcb07aab78fc89ce1e1ace68e6b19a6dbcc97d6d7981e08d91
          args:
            - -config.file=/etc/loki/loki.yaml
          ports:
            - name: http
              containerPort: 3100
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 15
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 30
            periodSeconds: 30
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          volumeMounts:
            - name: config
              mountPath: /etc/loki
            - name: storage
              mountPath: /loki
      volumes:
        - name: config
          configMap:
            name: loki-config
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes: [ReadWriteOnce]
        resources:
          requests:
            storage: 50Gi
---
apiVersion: v1
kind: Service
metadata:
  name: loki
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: loki
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 3100
      targetPort: http
  selector:
    app.kubernetes.io/name: loki
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: loki
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: loki
---
# ══════════════════════════════════════════════════════════════════
# 2. PROMTAIL — Log Collection DaemonSet
# ══════════════════════════════════════════════════════════════════
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: promtail
    app.kubernetes.io/component: log-collector
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      log_level: warn

    positions:
      filename: /run/promtail/positions.yaml

    clients:
      - url: http://loki.musd-canton:3100/loki/api/v1/push
        tenant_id: minted
        batchwait: 1s
        batchsize: 1048576  # 1MB

    scrape_configs:
      # Scrape all pod logs in musd-canton namespace
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: [musd-canton]
        relabel_configs:
          # Keep only musd-canton namespace pods
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: musd-canton
          # Use pod name as label
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          # Use container name as label
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
          # Use node name as label
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node
          # Add component label from pod labels
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
            target_label: component
          # Add app label from pod labels
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            target_label: app
        pipeline_stages:
          # Parse JSON logs (relay services, bot services emit structured JSON)
          - json:
              expressions:
                level: level
                msg: msg
                service: service
                error: error
          # Add parsed level as label
          - labels:
              level:
              service:
          # Drop debug logs in production to save storage
          - match:
              selector: '{level="debug"}'
              action: drop
              drop_counter_reason: debug_logs_dropped
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: promtail
    app.kubernetes.io/component: log-collector
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: promtail
  template:
    metadata:
      labels:
        app.kubernetes.io/name: promtail
    spec:
      serviceAccountName: promtail
      securityContext:
        runAsUser: 0  # Required to read container logs
      containers:
        - name: promtail
          image: grafana/promtail:3.3.2@sha256:2f1a8874dfa0073d22493197b2e3a4e3ea6bca426e4f3a0bca09e80c0eb1dfe8
          args:
            - -config.file=/etc/promtail/promtail.yaml
          ports:
            - name: http
              containerPort: 9080
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 128Mi
          volumeMounts:
            - name: config
              mountPath: /etc/promtail
            - name: run
              mountPath: /run/promtail
            # Mount container log directories
            - name: containers
              mountPath: /var/log/pods
              readOnly: true
            - name: docker
              mountPath: /var/lib/docker/containers
              readOnly: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
          operator: Exists
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: promtail-config
        - name: run
          hostPath:
            path: /run/promtail
        - name: containers
          hostPath:
            path: /var/log/pods
        - name: docker
          hostPath:
            path: /var/lib/docker/containers
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: promtail
---
# ClusterRole for Promtail to discover pods
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail-musd-canton
  labels:
    app.kubernetes.io/name: promtail
rules:
  - apiGroups: [""]
    resources: [nodes, nodes/proxy, services, endpoints, pods]
    verbs: [get, watch, list]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail-musd-canton
  labels:
    app.kubernetes.io/name: promtail
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: promtail-musd-canton
subjects:
  - kind: ServiceAccount
    name: promtail
    namespace: musd-canton
---
# ══════════════════════════════════════════════════════════════════
# 3. GRAFANA DATASOURCE — Auto-configure Loki in Grafana
# ══════════════════════════════════════════════════════════════════
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-loki-datasource
  namespace: musd-canton
  labels:
    app.kubernetes.io/name: grafana
    grafana_datasource: "true"
data:
  loki-datasource.yaml: |
    apiVersion: 1
    datasources:
      - name: Loki
        type: loki
        access: proxy
        url: http://loki.musd-canton:3100
        isDefault: false
        jsonData:
          maxLines: 1000
        editable: false
