# Minted Bridge - Docker Compose
# Use Docker secrets for sensitive values instead of env vars
# Added network isolation, resource limits, and read-only rootfs
# Health check binds to localhost
#
# Usage:
#   docker-compose up relay         # Start relay service only
#   docker-compose up validator1    # Start validator 1
#   docker-compose up               # Start everything

# Removed deprecated 'version' field (docker compose v2+ ignores it)

# Define secrets (files must be created on host)
secrets:
  canton_token:
    file: ./secrets/canton_token
  relayer_private_key:
    file: ./secrets/relayer_private_key
  # INFRA-H-03: RPC URLs contain API keys (Infura/Alchemy) — treat as secrets
  ethereum_rpc_url:
    file: ./secrets/ethereum_rpc_url
  validator1_aws_access_key:
    file: ./secrets/validator1_aws_access_key
  validator1_aws_secret_key:
    file: ./secrets/validator1_aws_secret_key
  validator2_aws_access_key:
    file: ./secrets/validator2_aws_access_key
  validator2_aws_secret_key:
    file: ./secrets/validator2_aws_secret_key
  validator3_aws_access_key:
    file: ./secrets/validator3_aws_access_key
  validator3_aws_secret_key:
    file: ./secrets/validator3_aws_secret_key

# Network isolation
networks:
  bridge_internal:
    driver: bridge
    internal: true
  bridge_external:
    driver: bridge

services:
  # ============================================================
  # RELAY SERVICE
  # Bridges finalized attestations from Canton to Ethereum
  # ============================================================
  relay:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["npm", "run", "relay:prod"]
    environment:
      - CANTON_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_PORT=${CANTON_PORT:-7575}
      - CANTON_USE_TLS=${CANTON_USE_TLS:-false}
      - CANTON_PARTY=${CANTON_PARTY}
      # INFRA-H-03: ETHEREUM_RPC_URL moved to secrets (contains API keys)
      # Read from /run/secrets/ethereum_rpc_url at runtime
      - BRIDGE_CONTRACT_ADDRESS=${BRIDGE_CONTRACT_ADDRESS}
      - CONFIRMATIONS=${CONFIRMATIONS:-2}
      - POLL_INTERVAL_MS=${POLL_INTERVAL_MS:-5000}
      - HEALTH_PORT=8080
      # Item-10: Bind health/metrics to 0.0.0.0 so Prometheus can scrape
      # within the docker network (bridge_internal). External access is
      # restricted by port mapping to 127.0.0.1 and network isolation.
      - HEALTH_BIND_HOST=0.0.0.0
      - NODE_ENV=${NODE_ENV:-development}
    # Mount secrets instead of env vars for sensitive values
    secrets:
      - canton_token
      - relayer_private_key
      - ethereum_rpc_url
    ports:
      - "127.0.0.1:8080:8080"
    restart: unless-stopped
    # Resource limits and security hardening
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
      - bridge_external
    # Use node instead of curl (not available in alpine)
    healthcheck:
      test: ["CMD", "node", "-e", "const http = require('http'); http.get('http://127.0.0.1:8080/health', (r) => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================
  # YIELD SYNC SERVICE
  # Unified cross-chain yield distribution
  # Syncs global share price between Ethereum SMUSD and Canton
  # ============================================================
  yield-sync:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["npm", "run", "yield-sync:prod"]
    environment:
      - CANTON_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_PORT=${CANTON_PORT:-7575}
      - CANTON_USE_TLS=${CANTON_USE_TLS:-false}
      - CANTON_PARTY=${CANTON_PARTY}
      # INFRA-H-03: ETHEREUM_RPC_URL moved to secrets (contains API keys)
      - TREASURY_ADDRESS=${TREASURY_ADDRESS}
      - SMUSD_ADDRESS=${SMUSD_ADDRESS}
      - YIELD_SYNC_INTERVAL_MS=${YIELD_SYNC_INTERVAL_MS:-3600000}
      - MIN_YIELD_THRESHOLD=${MIN_YIELD_THRESHOLD:-1000000000}
      - NODE_ENV=${NODE_ENV:-development}
    secrets:
      - canton_token
      - relayer_private_key
      - ethereum_rpc_url
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
      - bridge_external
    healthcheck:
      test: ["CMD", "node", "-e", "const fs=require('fs');try{const s=fs.statSync('/tmp/heartbeat');process.exit(Date.now()-s.mtimeMs<300000?0:1)}catch(e){process.exit(1)}"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================
  # VALIDATOR NODES
  # Each validator runs independently with their own KMS key
  # ============================================================
  validator1:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["npm", "run", "validator:prod"]
    environment:
      - CANTON_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_PORT=${CANTON_PORT:-7575}
      - CANTON_USE_TLS=${CANTON_USE_TLS:-false}
      # validator-node-v2.ts reads CANTON_LEDGER_HOST/PORT — alias them
      - CANTON_LEDGER_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_LEDGER_PORT=${CANTON_PORT:-7575}
      - VALIDATOR_PARTY=${VALIDATOR1_PARTY}
      - VALIDATOR_ETH_ADDRESS=${VALIDATOR1_ETH_ADDRESS}
      - KMS_KEY_ID=${VALIDATOR1_KMS_KEY_ID}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - BRIDGE_CONTRACT_ADDRESS=${BRIDGE_CONTRACT_ADDRESS}
      - MIN_COLLATERAL_RATIO_BPS=${MIN_COLLATERAL_RATIO_BPS:-11000}
      - POLL_INTERVAL_MS=${POLL_INTERVAL_MS:-3000}
      # Set NODE_ENV — use 'production' for mainnet (enforces TLS + KMS)
      - NODE_ENV=${NODE_ENV:-development}
    # Use secrets for AWS credentials
    secrets:
      - canton_token
      - validator1_aws_access_key
      - validator1_aws_secret_key
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    # Healthcheck for validator (process-level since no HTTP endpoint)
    healthcheck:
      test: ["CMD", "node", "-e", "const fs=require('fs');try{const s=fs.statSync('/tmp/heartbeat');process.exit(Date.now()-s.mtimeMs<120000?0:1)}catch(e){process.exit(1)}"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  validator2:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["npm", "run", "validator:prod"]
    environment:
      - CANTON_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_PORT=${CANTON_PORT:-7575}
      - CANTON_USE_TLS=${CANTON_USE_TLS:-false}
      # validator-node-v2.ts reads CANTON_LEDGER_HOST/PORT — alias them
      - CANTON_LEDGER_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_LEDGER_PORT=${CANTON_PORT:-7575}
      - VALIDATOR_PARTY=${VALIDATOR2_PARTY}
      - VALIDATOR_ETH_ADDRESS=${VALIDATOR2_ETH_ADDRESS}
      - KMS_KEY_ID=${VALIDATOR2_KMS_KEY_ID}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - BRIDGE_CONTRACT_ADDRESS=${BRIDGE_CONTRACT_ADDRESS}
      - MIN_COLLATERAL_RATIO_BPS=${MIN_COLLATERAL_RATIO_BPS:-11000}
      - POLL_INTERVAL_MS=${POLL_INTERVAL_MS:-3000}
      - NODE_ENV=${NODE_ENV:-development}
    secrets:
      - canton_token
      - validator2_aws_access_key
      - validator2_aws_secret_key
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    # Healthcheck for validator (process-level)
    healthcheck:
      test: ["CMD", "node", "-e", "const fs=require('fs');try{const s=fs.statSync('/tmp/heartbeat');process.exit(Date.now()-s.mtimeMs<120000?0:1)}catch(e){process.exit(1)}"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  validator3:
    build:
      context: .
      dockerfile: Dockerfile
    command: ["npm", "run", "validator:prod"]
    environment:
      - CANTON_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_PORT=${CANTON_PORT:-7575}
      - CANTON_USE_TLS=${CANTON_USE_TLS:-false}
      # validator-node-v2.ts reads CANTON_LEDGER_HOST/PORT — alias them
      - CANTON_LEDGER_HOST=${CANTON_HOST:-host.docker.internal}
      - CANTON_LEDGER_PORT=${CANTON_PORT:-7575}
      - VALIDATOR_PARTY=${VALIDATOR3_PARTY}
      - VALIDATOR_ETH_ADDRESS=${VALIDATOR3_ETH_ADDRESS}
      - KMS_KEY_ID=${VALIDATOR3_KMS_KEY_ID}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - BRIDGE_CONTRACT_ADDRESS=${BRIDGE_CONTRACT_ADDRESS}
      - MIN_COLLATERAL_RATIO_BPS=${MIN_COLLATERAL_RATIO_BPS:-11000}
      - POLL_INTERVAL_MS=${POLL_INTERVAL_MS:-3000}
      - NODE_ENV=${NODE_ENV:-development}
    secrets:
      - canton_token
      - validator3_aws_access_key
      - validator3_aws_secret_key
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    # Healthcheck for validator (process-level)
    healthcheck:
      test: ["CMD", "node", "-e", "const fs=require('fs');try{const s=fs.statSync('/tmp/heartbeat');process.exit(Date.now()-s.mtimeMs<120000?0:1)}catch(e){process.exit(1)}"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================================
  # MONITORING STACK (Item-10)
  # Prometheus + Grafana + Alertmanager
  # ============================================================
  prometheus:
    image: prom/prometheus:v2.53.0
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=30d"
      - "--web.enable-lifecycle"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    ports:
      - "127.0.0.1:9090:9090"
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    image: grafana/grafana:11.1.0
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-changeme}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    ports:
      - "127.0.0.1:3000:3000"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

  alertmanager:
    image: prom/alertmanager:v0.27.0
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "127.0.0.1:9093:9093"
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /tmp
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
    security_opt:
      - no-new-privileges:true
    networks:
      - bridge_internal
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"

# Persistent volumes for monitoring data
volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data: